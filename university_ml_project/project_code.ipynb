{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+PlTgL1qGWi2SQ5zA1H+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cecilia-cmd/2024_MLEES/blob/main/Final_Project/project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning Project - Comparing Machine Learning Algorithms for Predicting Biome Type from Soil Microbial Communities"
      ],
      "metadata": {
        "id": "8F3S7qX4SxrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------- IMPORTATION PACKAGES -------------------------------------\n",
        "#1.Prepare the data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#2. First method PCA\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#3. Second method KPCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "#Random Forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#SVM model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#KNN\n",
        "from sklearn.preprocessing import LabelEncoder#because KNN not recognize target labels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from scipy import stats\n",
        "\n",
        "#Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Results\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "metadata": {
        "id": "vm-MfUv-TWqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------- 1. PREPARE DATA -----------------------------------------\n",
        "#Load preprocessed data\n",
        "merged_data = pd.read_csv('/Users/ceciliatorres/Desktop/UNI/ML/Personal ML project/Data/preprocessed_data.csv')\n",
        "\n",
        "#Define features X and targets y\n",
        "otu_columns = [col for col in merged_data.columns if col.startswith('OTU_')]\n",
        "X = merged_data[otu_columns]  # OTU data as features\n",
        "y = merged_data['biome_clean']  # Target variable\n",
        "\n",
        "print(X.head())\n",
        "\n",
        "#Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(y_train.value_counts())\n",
        "\n",
        "###details for the splitting\n",
        "num_train_samples = X_train.shape[0]\n",
        "num_test_samples = X_test.shape[0]\n",
        "print(f\"Number of training samples: {num_train_samples}\")\n",
        "print(f\"Number of test samples: {num_test_samples}\")\n",
        "\n",
        "\n",
        "#Standardize the data on the training set only\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
        "X_test = scaler.transform(X_test)        # Transform the test data using the same scaler\n",
        "print(X_train[:5])\n"
      ],
      "metadata": {
        "id": "PJEyN3UUT2eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###------------ 2. DIMENSIONALITY REDUCTION FOR FEATURES -----------------------------------\n",
        "#------------------ A. FIRST METHOD PCA (linear) -------------------------------------------\n",
        "\n",
        "#Perform PCA without reducing dimensionality\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "#Compute the cumulative sum of explained variance ratio\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "#Find the number of components to preserve 95% variance\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "print(f\"Number of components to preserve 95% variance: {d}\")\n",
        "\n",
        "#to visualize the cumulative variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumsum, linewidth=1.5, color=\"slateblue\", label=\"Cumulative Variance\")\n",
        "plt.axvline(x=d, color='red', linestyle='-.', linewidth=1.8, label=f'{d} Components (95% Variance)')\n",
        "plt.xlabel(\"Number of Principal Components\", fontsize=13, labelpad=17)\n",
        "plt.ylabel(\"Cumulative Explained Variance\", fontsize=13, labelpad=17)\n",
        "#plt.title(\"Explained Variance vs. Number of Components\", fontsize=16, fontweight=\"bold\", pad=22)\n",
        "plt.legend(fontsize=12, loc='upper left')\n",
        "plt.grid(linewidth=0.4, linestyle=\"--\", alpha=0.6)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.tight_layout()\n",
        "#plt.savefig(\"explained_variance_pca.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# Now we can run PCA again with the optimal number of components\n",
        "pca = PCA(n_components=d)  # d= 422\n",
        "X_train_PCA = pca.fit_transform(X_train)\n",
        "\n",
        "#and we apply the same transformation to the test set\n",
        "X_test_PCA = pca.transform(X_test)\n",
        "\n",
        "print(\"Shape of X_train after PCA:\", X_train_PCA.shape)\n",
        "print(\"Shape of X_test after PCA:\", X_test_PCA.shape)\n"
      ],
      "metadata": {
        "id": "ISbiatVvW1pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------B. SECOND METHOD: KPCA ------------------------\n",
        "\n",
        "#Test KPCA, (we can play a bit with the parameters to check the changement in X_train_kpca)\n",
        "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.001, fit_inverse_transform=True)\n",
        "X_train_kpca = kpca.fit_transform(X_train)\n",
        "\n",
        "print(f\"Shape of X_train_kpca: {X_train_kpca.shape}\")\n",
        "print(X_train_kpca[:5])\n",
        "\n",
        "\n",
        "#Now how to choose the best number of components for KPCA?\n",
        "\n",
        "#we define the range of components to test\n",
        "components = range(1, 41)  #test from 1 to 41 components\n",
        "\n",
        "#we initialize an empty list to store the reconstruction errors\n",
        "reconstruction_errors = []\n",
        "\n",
        "#loop through the range of components\n",
        "for n in components:\n",
        "\n",
        "    kpca = KernelPCA(n_components=n, kernel=\"rbf\", gamma=0.001, fit_inverse_transform=True)\n",
        "    X_train_kpca = kpca.fit_transform(X_train)\n",
        "    X_train_reconstructed = kpca.inverse_transform(X_train_kpca)\n",
        "\n",
        "    #we calculate the reconstruction error\n",
        "    error = mean_squared_error(X_train, X_train_reconstructed)\n",
        "    reconstruction_errors.append(error)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(components, reconstruction_errors, marker='o')\n",
        "plt.title(\"Reconstruction Error vs Number of Components (RBF Kernel)\", fontsize=14)\n",
        "plt.xlabel(\"Number of Components\", fontsize=12)\n",
        "plt.ylabel(\"Reconstruction Error (MSE)\", fontsize=12)\n",
        "plt.xticks(components)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#we can observe on the figure that 20 number of components might be the best choice\n",
        "\n",
        "\n",
        "#scoring function\n",
        "def reconstruction_error_scorer(estimator, X):\n",
        "    X_transformed = estimator.transform(X)\n",
        "    X_reconstructed = estimator.inverse_transform(X_transformed)\n",
        "    return -mean_squared_error(X, X_reconstructed)  #negative because GridSearchCV maximizes score\n",
        "\n",
        "'''\n",
        "The following sections document the parameter tuning experiments I conducted.\n",
        "These tests aimed to optimize the `gamma` parameter and the number of components (`n_components`)\n",
        "for the KernelPCA model using a systematic grid search approach.\n",
        "\n",
        "--- First Experiment: Tuning `gamma` within a smaller range ---\n",
        "Initial parameter grid to narrow down the best `gamma`:\n",
        "Here, I refined the search range based on a previous experiment where `gamma = 0.001` performed well.\n",
        "\n",
        "param_grid = {\n",
        "    'gamma': np.linspace(0.0001, 0.001, 10),  # Test gamma values between 0.0001 and 0.001.\n",
        "    'n_components': [20]  # Fix the number of components for simplicity.\n",
        "}\n",
        "\n",
        "# Initialize KernelPCA\n",
        "kpca = KernelPCA(kernel='rbf', fit_inverse_transform=True)\n",
        "\n",
        "# Initialize GridSearchCV with 3-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=kpca,\n",
        "    param_grid=param_grid,\n",
        "    scoring=reconstruction_error_scorer,  # Custom scoring function for reconstruction error\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train)\n",
        "\n",
        "# Print the best `gamma` value and its corresponding reconstruction error\n",
        "print(f\"Best Gamma: {grid_search.best_params_['gamma']}\")  # Example output: 0.0001\n",
        "print(f\"Best Score (Reconstruction Error): {-grid_search.best_score_}\")  # Convert back to positive MSE\n",
        "\n",
        "--- Second Experiment: Joint tuning of `gamma` and `n_components` ---\n",
        "Refine `gamma` around the previously identified optimal value (0.0001)\n",
        "and experiment with multiple values of `n_components`:\n",
        "\n",
        "param_grid = {\n",
        "    'gamma': np.linspace(0.00005, 0.0002, 10),  # Fine-tuned range around 0.0001.\n",
        "    'n_components': [10, 15, 20, 25, 30, 35, 40]  # Test different component numbers.\n",
        "}\n",
        "\n",
        "# Initialize KernelPCA\n",
        "kpca = KernelPCA(kernel='rbf', fit_inverse_transform=True)\n",
        "\n",
        "# Initialize GridSearchCV with 3-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=kpca,\n",
        "    param_grid=param_grid,\n",
        "    scoring=reconstruction_error_scorer,\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Utilize all available cores for computation\n",
        ")\n",
        "\n",
        "# Fit the grid search on training data\n",
        "grid_search.fit(X_train)\n",
        "\n",
        "# Print the best parameters and corresponding reconstruction error\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")  # Example: {'gamma': 6.67e-05, 'n_components': 40}\n",
        "print(f\"Best Score (Reconstruction Error): {-grid_search.best_score_}\")  # Example: 0.9518\n",
        "\n",
        "'''\n",
        "\n",
        "##now we can fine tune the model one last time :\n",
        "\n",
        "param_grid = {\n",
        "    'gamma': np.linspace(6.5e-05, 7.5e-05, 10),\n",
        "    'n_components': range(35, 45, 2)\n",
        "}\n",
        "\n",
        "\n",
        "#initialization of the KernelPCA estimator\n",
        "kpca = KernelPCA(kernel='rbf', fit_inverse_transform=True)\n",
        "\n",
        "#initialization ofthe GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=kpca,\n",
        "    param_grid=param_grid,\n",
        "    scoring=reconstruction_error_scorer,\n",
        "    cv=10,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "#fit grid search on training data\n",
        "grid_search.fit(X_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\") #Best Parameters: {'gamma': 6.5e-05, 'n_components': 43}\n",
        "print(f\"Best Score (Reconstruction Error): {-grid_search.best_score_}\")\n",
        "\n",
        "best_kpca = KernelPCA(n_components=43, kernel=\"rbf\", gamma=6.5e-05, fit_inverse_transform=True)\n",
        "X_train_kpca = best_kpca.fit_transform(X_train)\n",
        "X_test_kpca = best_kpca.transform(X_test)\n",
        "\n",
        "print(\"Shape of X_train after KPCA:\", X_train_kpca.shape)\n",
        "print(\"Shape of X_test after KPCA:\", X_test_kpca.shape)\n",
        "\n",
        "\n",
        "###Visualization for choice of #components KPCA\n",
        "#we define the best gamma\n",
        "best_gamma = 6.5e-05\n",
        "\n",
        "#range of components to test\n",
        "n_components_range = range(1, 51)\n",
        "\n",
        "#list to store reconstruction errors\n",
        "reconstruction_errors = []\n",
        "\n",
        "#loop over different numbers of components\n",
        "for n_components in n_components_range:\n",
        "    # Create and fit KernelPCA with the current number of components\n",
        "    kpca = KernelPCA(n_components=n_components, kernel=\"rbf\", gamma=best_gamma, fit_inverse_transform=True)\n",
        "    X_train_kpca = kpca.fit_transform(X_train)\n",
        "    X_train_reconstructed = kpca.inverse_transform(X_train_kpca)\n",
        "\n",
        "    #reconstruction error\n",
        "    error = mean_squared_error(X_train, X_train_reconstructed)\n",
        "    reconstruction_errors.append(error)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_components_range, reconstruction_errors, color=\"slateblue\", linewidth=1.5, marker='o', linestyle='-', label=\"Reconstruction Error\")\n",
        "plt.axvline(x=43, color='red', linestyle='-.', linewidth=1.8, label=\"Best n_components (43)\")\n",
        "plt.xlabel(\"Number of Components\", fontsize=13, labelpad=17)\n",
        "plt.ylabel(\"Reconstruction Error\", fontsize=13, labelpad=17)\n",
        "#plt.title(f\"Reconstruction Error vs. Number of Components (gamma={best_gamma:.2e})\",  fontsize=16, fontweight=\"bold\", pad=22)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(linewidth=0.4, linestyle=\"--\", alpha=0.6)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.tight_layout()\n",
        "#plt.savefig(\"Reconstruction_error_KPCA.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NP9mNAf_XBvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our reduced data with PCA and KPCA we can begin our comparison of machine learning models for each PCA and KPCA.\n",
        "Let's begin with PCA data."
      ],
      "metadata": {
        "id": "pknIZnEKf35v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- A.Softmax Regression -------------------- #\n",
        "\n",
        "#we define the Softmax Regression model\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2000)\n",
        "\n",
        "#then we define the hyperparameter grid for Softmax Regression\n",
        "param_grid = {\n",
        "    \"C\": [0.01, 0.1, 1, 10, 100]  #regularization strength\n",
        "}\n",
        "\n",
        "#GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(softmax_reg, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train_PCA, y_train)\n",
        "\n",
        "#best parameters and cross-validation accuracy\n",
        "print(\"Best Softmax Regression parameters:\", grid_search.best_params_) #{'C': 10}\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)#0.9794216726298831\n",
        "\n",
        "#we evaluate the best model on the test set\n",
        "PCA_Softmax_model = grid_search.best_estimator_\n",
        "PCA_Softmax_predictions = PCA_Softmax_model.predict(X_test_PCA)\n",
        "PCA_Softmax_accuracy = accuracy_score(y_test, PCA_Softmax_predictions)\n",
        "print(f\"PCA + Softmax Regression Test Set Accuracy: {PCA_Softmax_accuracy:.4f}\")\n",
        "cm = confusion_matrix(y_test, PCA_Softmax_predictions)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "\n",
        "##we evaluate the performance metrics on training set :\n",
        "train_predictions = PCA_Softmax_model.predict(X_train_PCA)\n",
        "train_probas = PCA_Softmax_model.predict_proba(X_train_PCA)      #class probabilities\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "train_f1 = f1_score(y_train, train_predictions, average='weighted')\n",
        "train_log_loss = log_loss(y_train, train_probas)  #log Loss\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
        "print(f\"Training Log Loss: {train_log_loss:.4f}\")\n",
        "\n",
        "\n",
        "#we perform cross-validation predictions\n",
        "y_pred_cv = cross_val_predict(PCA_Softmax_model, X_train_PCA, y_train, cv=3, method='predict')\n",
        "y_proba_cv = cross_val_predict(PCA_Softmax_model, X_train_PCA, y_train, cv=3, method='predict_proba')\n",
        "\n",
        "cv_accuracy = accuracy_score(y_train, y_pred_cv)\n",
        "cv_f1 = f1_score(y_train, y_pred_cv, average='weighted')\n",
        "cv_log_loss = log_loss(y_train, y_proba_cv)\n",
        "\n",
        "print(f\"Cross-Validation Accuracy: {cv_accuracy:.4f}\")\n",
        "print(f\"Cross-Validation F1 Score: {cv_f1:.4f}\")\n",
        "print(f\"Cross-Validation Log Loss: {cv_log_loss:.4f}\")\n",
        "\n",
        "\n",
        "#now predictions and probabilities on the test set\n",
        "test_predictions = PCA_Softmax_model.predict(X_test_PCA)\n",
        "test_probas = PCA_Softmax_model.predict_proba(X_test_PCA)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions, average='weighted')\n",
        "test_log_loss = log_loss(y_test, test_probas)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Test Log Loss: {test_log_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, PCA_Softmax_predictions))\n"
      ],
      "metadata": {
        "id": "wq7KT7JQbbJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- B.Random Forest Classifier -------------------- #\n",
        "\n",
        "#we define the Random Forest Classifier model\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "#Then the hyperparameter grid for Random Forest\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],  #number of trees in the forest\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(rf_clf, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train_PCA, y_train)\n",
        "\n",
        "print(\"Best Random Forest parameters:\", grid_search.best_params_) #{'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50}\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_) # 0.9027681878099303\n",
        "\n",
        "#we evaluate the best model on the test set\n",
        "PCA_RF_model = grid_search.best_estimator_\n",
        "\n",
        "#we evaluate the best model on the training set\n",
        "train_predictions_RF = PCA_RF_model.predict(X_train_PCA)\n",
        "train_probas_RF = PCA_RF_model.predict_proba(X_train_PCA)\n",
        "train_accuracy_RF = accuracy_score(y_train, train_predictions_RF)\n",
        "train_f1_RF = f1_score(y_train, train_predictions_RF, average='weighted')\n",
        "train_log_loss_RF = log_loss(y_train, train_probas_RF)\n",
        "\n",
        "print(f\"[Random Forest - PCA] Training Accuracy: {train_accuracy_RF:.4f}\")\n",
        "print(f\"[Random Forest - PCA] Training F1 Score: {train_f1_RF:.4f}\")\n",
        "print(f\"[Random Forest - PCA] Training Log Loss: {train_log_loss_RF:.4f}\")\n",
        "\n",
        "#now cross-validation\n",
        "y_pred_cv_RF = cross_val_predict(PCA_RF_model, X_train_PCA, y_train, cv=3, method='predict')\n",
        "y_proba_cv_RF = cross_val_predict(PCA_RF_model, X_train_PCA, y_train, cv=3, method='predict_proba')\n",
        "cv_accuracy_RF = accuracy_score(y_train, y_pred_cv_RF)\n",
        "cv_f1_RF = f1_score(y_train, y_pred_cv_RF, average='weighted')\n",
        "cv_log_loss_RF = log_loss(y_train, y_proba_cv_RF)\n",
        "\n",
        "print(f\"[Random Forest - PCA] CV Accuracy: {cv_accuracy_RF:.4f}\")\n",
        "print(f\"[Random Forest - PCA] CV F1 Score: {cv_f1_RF:.4f}\")\n",
        "print(f\"[Random Forest - PCA] CV Log Loss: {cv_log_loss_RF:.4f}\")\n",
        "\n",
        "#and test set\n",
        "test_predictions_RF = PCA_RF_model.predict(X_test_PCA)\n",
        "test_probas_RF = PCA_RF_model.predict_proba(X_test_PCA)\n",
        "test_accuracy_RF = accuracy_score(y_test, test_predictions_RF)\n",
        "test_f1_RF = f1_score(y_test, test_predictions_RF, average='weighted')\n",
        "test_log_loss_RF = log_loss(y_test, test_probas_RF)\n",
        "\n",
        "print(f\"[Random Forest - PCA] Test Accuracy: {test_accuracy_RF:.4f}\")\n",
        "print(f\"[Random Forest - PCA] Test F1 Score: {test_f1_RF:.4f}\")\n",
        "print(f\"[Random Forest - PCA] Test Log Loss: {test_log_loss_RF:.4f}\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, PCA_RF_predictions))\n"
      ],
      "metadata": {
        "id": "zqbGzxGlbhsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- C.KNN -------------------- #\n",
        "\n",
        "#first encode the target variable (because feature = categorical)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)  #fit and transform for training set\n",
        "y_test_encoded = label_encoder.transform(y_test)        #transform for test set\n",
        "\n",
        "# we define the KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "#now the hyperparameter grid for KNN\n",
        "param_grid = {\n",
        "    \"n_neighbors\": [3, 5, 7, 9],      # Number of neighbors\n",
        "    \"weights\": [\"uniform\", \"distance\"],  # Weighting scheme\n",
        "    \"metric\": [\"euclidean\", \"manhattan\"]  # Distance metrics\n",
        "}\n",
        "\n",
        "#we perform GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train_PCA, y_train_encoded)\n",
        "\n",
        "print(\"Best KNN parameters:\", grid_search.best_params_) #{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)#0.7757202937668697\n",
        "\n",
        "PCA_KNN_model = grid_search.best_estimator_\n",
        "\n",
        "#we evaluate the best model on the training set\n",
        "train_predictions_KNN = PCA_KNN_model.predict(X_train_PCA)\n",
        "train_probas_KNN = PCA_KNN_model.predict_proba(X_train_PCA)\n",
        "train_accuracy_KNN = accuracy_score(y_train_encoded, train_predictions_KNN)\n",
        "train_f1_KNN = f1_score(y_train_encoded, train_predictions_KNN, average='weighted')\n",
        "train_log_loss_KNN = log_loss(y_train_encoded, train_probas_KNN)\n",
        "\n",
        "print(f\"[KNN - PCA] Training Accuracy: {train_accuracy_KNN:.4f}\")\n",
        "print(f\"[KNN - PCA] Training F1 Score: {train_f1_KNN:.4f}\")\n",
        "print(f\"[KNN - PCA] Training Log Loss: {train_log_loss_KNN:.4f}\")\n",
        "\n",
        "#now cross validation\n",
        "y_pred_cv_KNN = cross_val_predict(PCA_KNN_model, X_train_PCA, y_train_encoded, cv=3, method='predict')\n",
        "y_proba_cv_KNN = cross_val_predict(PCA_KNN_model, X_train_PCA, y_train_encoded, cv=3, method='predict_proba')\n",
        "cv_accuracy_KNN = accuracy_score(y_train_encoded, y_pred_cv_KNN)\n",
        "cv_f1_KNN = f1_score(y_train_encoded, y_pred_cv_KNN, average='weighted')\n",
        "cv_log_loss_KNN = log_loss(y_train_encoded, y_proba_cv_KNN)\n",
        "\n",
        "print(f\"[KNN - PCA] CV Accuracy: {cv_accuracy_KNN:.4f}\")\n",
        "print(f\"[KNN - PCA] CV F1 Score: {cv_f1_KNN:.4f}\")\n",
        "print(f\"[KNN - PCA] CV Log Loss: {cv_log_loss_KNN:.4f}\")\n",
        "\n",
        "#and test set\n",
        "test_predictions_KNN = PCA_KNN_model.predict(X_test_PCA)\n",
        "test_probas_KNN = PCA_KNN_model.predict_proba(X_test_PCA)\n",
        "test_accuracy_KNN = accuracy_score(y_test_encoded, test_predictions_KNN)\n",
        "test_f1_KNN = f1_score(y_test_encoded, test_predictions_KNN, average='weighted')\n",
        "test_log_loss_KNN = log_loss(y_test_encoded, test_probas_KNN)\n",
        "\n",
        "print(f\"[KNN - PCA] Test Accuracy: {test_accuracy_KNN:.4f}\")\n",
        "print(f\"[KNN - PCA] Test F1 Score: {test_f1_KNN:.4f}\")\n",
        "print(f\"[KNN - PCA] Test Log Loss: {test_log_loss_KNN:.4f}\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_encoded, PCA_KNN_predictions, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "w-Ly7c1sbmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's do the second method with our KPCA data."
      ],
      "metadata": {
        "id": "fZzAdiSXgd1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###----------------- A. Softmax Regression -------------------------------------------\n",
        "\n",
        "#define the Softmax Regression model\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2000)\n",
        "\n",
        "#define the hyperparameter grid for Softmax Regression\n",
        "param_grid = {\n",
        "    \"C\": [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "#GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(softmax_reg, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1) #{'C': 100}\n",
        "grid_search.fit(X_train_kpca, y_train)\n",
        "\n",
        "print(\"Best Softmax Regression parameters:\", grid_search.best_params_)#{'C': 100}\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)#0.977549013453853\n",
        "\n",
        "\n",
        "KPCA_Softmax_model = grid_search.best_estimator_\n",
        "\n",
        "#we evaluate on the training set\n",
        "train_predictions_Softmax = KPCA_Softmax_model.predict(X_train_kpca)\n",
        "train_probas_Softmax = KPCA_Softmax_model.predict_proba(X_train_kpca)\n",
        "train_accuracy_Softmax = accuracy_score(y_train, train_predictions_Softmax)\n",
        "train_f1_Softmax = f1_score(y_train, train_predictions_Softmax, average='weighted')\n",
        "train_log_loss_Softmax = log_loss(y_train, train_probas_Softmax)\n",
        "\n",
        "print(f\"[Softmax - KPCA] Training Accuracy: {train_accuracy_Softmax:.4f}\")\n",
        "print(f\"[Softmax - KPCA] Training F1 Score: {train_f1_Softmax:.4f}\")\n",
        "print(f\"[Softmax - KPCA] Training Log Loss: {train_log_loss_Softmax:.4f}\")\n",
        "\n",
        "#now cross validation\n",
        "y_pred_cv_Softmax = cross_val_predict(KPCA_Softmax_model, X_train_kpca, y_train, cv=3, method='predict')\n",
        "y_proba_cv_Softmax = cross_val_predict(KPCA_Softmax_model, X_train_kpca, y_train, cv=3, method='predict_proba')\n",
        "cv_accuracy_Softmax = accuracy_score(y_train, y_pred_cv_Softmax)\n",
        "cv_f1_Softmax = f1_score(y_train, y_pred_cv_Softmax, average='weighted')\n",
        "cv_log_loss_Softmax = log_loss(y_train, y_proba_cv_Softmax)\n",
        "\n",
        "print(f\"[Softmax - KPCA] CV Accuracy: {cv_accuracy_Softmax:.4f}\")\n",
        "print(f\"[Softmax - KPCA] CV F1 Score: {cv_f1_Softmax:.4f}\")\n",
        "print(f\"[Softmax - KPCA] CV Log Loss: {cv_log_loss_Softmax:.4f}\")\n",
        "\n",
        "#and test set\n",
        "test_predictions_Softmax = KPCA_Softmax_model.predict(X_test_kpca)\n",
        "test_probas_Softmax = KPCA_Softmax_model.predict_proba(X_test_kpca)\n",
        "test_accuracy_Softmax = accuracy_score(y_test, test_predictions_Softmax)\n",
        "test_f1_Softmax = f1_score(y_test, test_predictions_Softmax, average='weighted')\n",
        "test_log_loss_Softmax = log_loss(y_test, test_probas_Softmax)\n",
        "\n",
        "print(f\"[Softmax - KPCA] Test Accuracy: {test_accuracy_Softmax:.4f}\")\n",
        "print(f\"[Softmax - KPCA] Test F1 Score: {test_f1_Softmax:.4f}\")\n",
        "print(f\"[Softmax - KPCA] Test Log Loss: {test_log_loss_Softmax:.4f}\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, KPCA_Softmax_predictions))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfvzSlk8bq3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------- B. Random Forest  -------------------------------------------\n",
        "\n",
        "\n",
        "#define the Random Forest Classifier model\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "#now the hyperparameter grid for Random Forest\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "#GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(rf_clf, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train_kpca, y_train)\n",
        "\n",
        "print(\"Best Random Forest parameters:\", grid_search.best_params_) #{'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_) #0.960716004435796\n",
        "\n",
        "\n",
        "KPCA_RF_model = grid_search.best_estimator_\n",
        "\n",
        "#we evaluate on the training set\n",
        "train_predictions_RF_KPCA = KPCA_RF_model.predict(X_train_kpca)\n",
        "train_probas_RF_KPCA = KPCA_RF_model.predict_proba(X_train_kpca)\n",
        "train_accuracy_RF_KPCA = accuracy_score(y_train, train_predictions_RF_KPCA)\n",
        "train_f1_RF_KPCA = f1_score(y_train, train_predictions_RF_KPCA, average='weighted')\n",
        "train_log_loss_RF_KPCA = log_loss(y_train, train_probas_RF_KPCA)\n",
        "\n",
        "print(f\"[Random Forest - KPCA] Training Accuracy: {train_accuracy_RF_KPCA:.4f}\")\n",
        "print(f\"[Random Forest - KPCA] Training F1 Score: {train_f1_RF_KPCA:.4f}\")\n",
        "print(f\"[Random Forest - KPCA] Training Log Loss: {train_log_loss_RF_KPCA:.4f}\")\n",
        "\n",
        "#now crossvalidation\n",
        "y_pred_cv_RF_KPCA = cross_val_predict(KPCA_RF_model, X_train_kpca, y_train, cv=3, method='predict')\n",
        "y_proba_cv_RF_KPCA = cross_val_predict(KPCA_RF_model, X_train_kpca, y_train, cv=3, method='predict_proba')\n",
        "cv_accuracy_RF_KPCA = accuracy_score(y_train, y_pred_cv_RF_KPCA)\n",
        "cv_f1_RF_KPCA = f1_score(y_train, y_pred_cv_RF_KPCA, average='weighted')\n",
        "cv_log_loss_RF_KPCA = log_loss(y_train, y_proba_cv_RF_KPCA)\n",
        "\n",
        "print(f\"[Random Forest - KPCA] CV Accuracy: {cv_accuracy_RF_KPCA:.4f}\")\n",
        "print(f\"[Random Forest - KPCA] CV F1 Score: {cv_f1_RF_KPCA:.4f}\")\n",
        "print(f\"[Random Forest - KPCA] CV Log Loss: {cv_log_loss_RF_KPCA:.4f}\")\n",
        "\n",
        "#and test set\n",
        "test_predictions_RF_KPCA = KPCA_RF_model.predict(X_test_kpca)\n",
        "test_probas_RF_KPCA = KPCA_RF_model.predict_proba(X_test_kpca)\n",
        "test_accuracy_RF_KPCA = accuracy_score(y_test, test_predictions_RF_KPCA)\n",
        "test_f1_RF_KPCA = f1_score(y_test, test_predictions_RF_KPCA, average='weighted')\n",
        "test_log_loss_RF_KPCA = log_loss(y_test, test_probas_RF_KPCA)\n",
        "\n",
        "print(f\"[Random Forest - KPCA] Test Accuracy: {test_accuracy_RF_KPCA:.4f}\")\n",
        "print(f\"[Random Forest - KPCA] Test F1 Score: {test_f1_RF_KPCA:.4f}\")\n",
        "print(f\"[Random Forest - KPCA] Test Log Loss: {test_log_loss_RF_KPCA:.4f}\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, KPCA_RF_predictions))\n"
      ],
      "metadata": {
        "id": "edgFAtbibvnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#----------------- C. KNN  -------------------------------------------\n",
        "\n",
        "#first we encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "#then we define the KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "#and the hyperparameter grid for KNN\n",
        "param_grid = {\n",
        "    \"n_neighbors\": [3, 5, 7, 9],      #number of neighbors\n",
        "    \"weights\": [\"uniform\", \"distance\"],  #weighting scheme\n",
        "    \"metric\": [\"euclidean\", \"manhattan\"]  #distance metrics\n",
        "}\n",
        "\n",
        "#GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train_kpca, y_train_encoded)\n",
        "\n",
        "print(\"Best KNN parameters:\", grid_search.best_params_) #{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_) #0.9756972778440357\n",
        "\n",
        "\n",
        "KPCA_KNN_model = grid_search.best_estimator_\n",
        "\n",
        "#we evaluate on the training set\n",
        "train_predictions_KNN_KPCA = KPCA_KNN_model.predict(X_train_kpca)\n",
        "train_probas_KNN_KPCA = KPCA_KNN_model.predict_proba(X_train_kpca)\n",
        "train_accuracy_KNN_KPCA = accuracy_score(y_train_encoded, train_predictions_KNN_KPCA)\n",
        "train_f1_KNN_KPCA = f1_score(y_train_encoded, train_predictions_KNN_KPCA, average='weighted')\n",
        "train_log_loss_KNN_KPCA = log_loss(y_train_encoded, train_probas_KNN_KPCA)\n",
        "\n",
        "print(f\"[KNN - KPCA] Training Accuracy: {train_accuracy_KNN_KPCA:.4f}\")\n",
        "print(f\"[KNN - KPCA] Training F1 Score: {train_f1_KNN_KPCA:.4f}\")\n",
        "print(f\"[KNN - KPCA] Training Log Loss: {train_log_loss_KNN_KPCA:.4f}\")\n",
        "\n",
        "#now cross validation\n",
        "y_pred_cv_KNN_KPCA = cross_val_predict(KPCA_KNN_model, X_train_kpca, y_train_encoded, cv=3, method='predict')\n",
        "y_proba_cv_KNN_KPCA = cross_val_predict(KPCA_KNN_model, X_train_kpca, y_train_encoded, cv=3, method='predict_proba')\n",
        "cv_accuracy_KNN_KPCA = accuracy_score(y_train_encoded, y_pred_cv_KNN_KPCA)\n",
        "cv_f1_KNN_KPCA = f1_score(y_train_encoded, y_pred_cv_KNN_KPCA, average='weighted')\n",
        "cv_log_loss_KNN_KPCA = log_loss(y_train_encoded, y_proba_cv_KNN_KPCA)\n",
        "\n",
        "print(f\"[KNN - KPCA] CV Accuracy: {cv_accuracy_KNN_KPCA:.4f}\")\n",
        "print(f\"[KNN - KPCA] CV F1 Score: {cv_f1_KNN_KPCA:.4f}\")\n",
        "print(f\"[KNN - KPCA] CV Log Loss: {cv_log_loss_KNN_KPCA:.4f}\")\n",
        "\n",
        "#and testing set\n",
        "test_predictions_KNN_KPCA = KPCA_KNN_model.predict(X_test_kpca)\n",
        "test_probas_KNN_KPCA = KPCA_KNN_model.predict_proba(X_test_kpca)\n",
        "test_accuracy_KNN_KPCA = accuracy_score(y_test_encoded, test_predictions_KNN_KPCA)\n",
        "test_f1_KNN_KPCA = f1_score(y_test_encoded, test_predictions_KNN_KPCA, average='weighted')\n",
        "test_log_loss_KNN_KPCA = log_loss(y_test_encoded, test_probas_KNN_KPCA)\n",
        "\n",
        "print(f\"[KNN - KPCA] Test Accuracy: {test_accuracy_KNN_KPCA:.4f}\")\n",
        "print(f\"[KNN - KPCA] Test F1 Score: {test_f1_KNN_KPCA:.4f}\")\n",
        "print(f\"[KNN - KPCA] Test Log Loss: {test_log_loss_KNN_KPCA:.4f}\")\n",
        "\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_encoded, KPCA_KNN_predictions, target_names=label_encoder.classes_))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnvN9AhIb4_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We executed all models for PCA and KPCA, so now let's compare them with ROC curves."
      ],
      "metadata": {
        "id": "a7GH25iqg2Ec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siisUA-5Svnb"
      },
      "outputs": [],
      "source": [
        "#-------------COMPARISON WITH ROC CURVES -------------------------------------\n",
        "\n",
        "\n",
        "#we binarize the labels for multiclass ROC\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))  #shape: (n_samples, n_classes)\n",
        "n_classes = y_test_bin.shape[1]\n",
        "\n",
        "#we create a dictionary to store AUC scores\n",
        "roc_auc_scores = {}\n",
        "\n",
        "#function to calculate ROC curves for each model\n",
        "def plot_roc_curve_multiclass(model, X_test, y_test_bin, label, color):\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(n_classes):\n",
        "        #predict probabilities for the positive class\n",
        "        y_proba = model.predict_proba(X_test)[:, i]\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    #average AUC across classes\n",
        "    roc_auc_scores[label] = np.mean(list(roc_auc.values()))\n",
        "    #plot the micro-average ROC curve\n",
        "    plt.plot(fpr[1], tpr[1], label=f\"{label} (AUC = {roc_auc_scores[label]:.2f})\", color=color)\n",
        "\n",
        "#plot all ROC curves\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Softmax Regression\n",
        "plot_roc_curve_multiclass(PCA_Softmax_model, X_test_PCA, y_test_bin, \"Softmax Regression (PCA)\", color=\"slateblue\")\n",
        "plt.plot([0], [0], color=\"slateblue\", linewidth=3, label=\"_nolegend_\")\n",
        "# Random Forest\n",
        "plot_roc_curve_multiclass(PCA_RF_model, X_test_PCA, y_test_bin, \"Random Forest (PCA)\", color=\"forestgreen\")\n",
        "plt.plot([0], [0], color=\"forestgreen\", linewidth=3, label=\"_nolegend_\")\n",
        "# KNN\n",
        "plot_roc_curve_multiclass(PCA_KNN_model, X_test_PCA, y_test_bin, \"KNN (PCA)\", color=\"orchid\")\n",
        "plt.plot([0], [0], color=\"orchid\", linewidth=3, label=\"_nolegend_\")\n",
        "#diagonal line\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", linewidth=1.5)\n",
        "#plt.title(\"ROC Curves for Softmax Regression, Random Forest, and KNN (PCA)\", fontsize=16, fontweight=\"bold\", pad=22)\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=13, labelpad=17)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=13, labelpad=17)\n",
        "plt.legend(loc=\"lower right\",fontsize=12)\n",
        "plt.grid(linewidth=0.4, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "#plt.savefig(\"ROC curves_PCA.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# Print the AUC scores for each model\n",
        "print(\"AUC Scores:\")\n",
        "for model_name, auc_score in roc_auc_scores.items():\n",
        "    print(f\"{model_name}: {auc_score:.2f}\")\n",
        "\n",
        "\n",
        "#Evaluation of ROC AUC\n",
        "#perform 5-fold cross-validation and evaluate multiclass ROC AUC (one-vs-rest strategy) for Softmax REgression\n",
        "scores_softmax = cross_val_score(PCA_Softmax_model, X_train_PCA, y_train, cv=5, scoring='roc_auc_ovr')\n",
        "\n",
        "print(\"Cross-Validation AUC Scores (Multiclass, OvR):\", scores_softmax)\n",
        "print(\"Mean CV AUC:\", scores_softmax.mean())\n",
        "print(\"Standard Deviation of CV AUC:\", scores_softmax.std())\n",
        "\n",
        "#cross-validation for Random Forest (PCA)\n",
        "scores_rf = cross_val_score(PCA_RF_model, X_train_PCA, y_train, cv=5, scoring='roc_auc_ovr')\n",
        "print(\"Random Forest Cross-Validation AUC Scores (Multiclass, OvR):\", scores_rf)\n",
        "print(\"Mean CV AUC (Random Forest):\", scores_rf.mean())\n",
        "print(\"Standard Deviation of CV AUC (Random Forest):\", scores_rf.std())\n",
        "\n",
        "#cross-validation for KNN (PCA)\n",
        "scores_knn = cross_val_score(PCA_KNN_model, X_train_PCA, y_train_encoded, cv=5, scoring='roc_auc_ovr')\n",
        "print(\"KNN Cross-Validation AUC Scores (Multiclass, OvR):\", scores_knn)\n",
        "print(\"Mean CV AUC (KNN):\", scores_knn.mean())\n",
        "print(\"Standard Deviation of CV AUC (KNN):\", scores_knn.std())\n",
        "\n",
        "\n",
        "###ROC curves for KPCA :\n",
        "\n",
        "#plot all ROC curves\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Softmax Regression\n",
        "plot_roc_curve_multiclass(KPCA_Softmax_model, X_test_kpca, y_test_bin, \"Softmax Regression (KPCA)\", color=\"slateblue\")\n",
        "plt.plot([0], [0], color=\"slateblue\", linewidth=3, label=\"_nolegend_\")\n",
        "# Random Forest\n",
        "plot_roc_curve_multiclass(KPCA_RF_model, X_test_kpca, y_test_bin, \"Random Forest (KPCA)\", color=\"forestgreen\")\n",
        "plt.plot([0], [0], color=\"forestgreen\", linewidth=3, label=\"_nolegend_\")\n",
        "# KNN\n",
        "plot_roc_curve_multiclass(KPCA_KNN_model, X_test_kpca, y_test_bin, \"KNN (KPCA)\", color=\"orchid\")\n",
        "plt.plot([0], [0], color=\"orchid\", linewidth=3, label=\"_nolegend_\")\n",
        "#diagonal\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", linewidth=1.5)\n",
        "\n",
        "#final plot\n",
        "#plt.title(\"ROC Curves for Softmax Regression, Random Forest, and KNN (KPCA)\", fontsize=16, fontweight=\"bold\", pad=22)\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=13, labelpad=17)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=13, labelpad=17)\n",
        "plt.legend(loc=\"lower right\",fontsize=12)\n",
        "plt.grid(linewidth=0.4, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "#plt.savefig(\"ROC curves_KPCA.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# Print the AUC scores for each model\n",
        "print(\"AUC Scores:\")\n",
        "for model_name, auc_score in roc_auc_scores.items():\n",
        "    print(f\"{model_name}: {auc_score:.2f}\")\n",
        "\n"
      ]
    }
  ]
}